{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:43:33.943007Z",
     "start_time": "2024-06-02T18:43:33.866631Z"
    }
   },
   "source": [
    "import sklearn\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from pmdarima.arima import auto_arima\n",
    "#from keras.models import load_model\n",
    "import joblib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, LSTM\n",
    "# from keras.models import load_model\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "from catboost import CatBoostRegressor\n",
    "import pandas as pd"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "http://localhost:8070"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5981aaea83ba5f55"
  },
  {
   "cell_type": "code",
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "#https://repo1.maven.org/maven2/com/github/housepower/clickhouse-native-jdbc/2.7.1/clickhouse-native-jdbc-2.7.1.jar\n",
    "packages = [\n",
    "    \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\",\n",
    "    \"com.clickhouse:clickhouse-jdbc:0.6.0-patch4\",\n",
    "    \"com.clickhouse:clickhouse-http-client:0.6.0-patch4\",\n",
    "    \"org.apache.httpcomponents.client5:httpclient5:5.3.1\",\n",
    "    \"com.github.housepower:clickhouse-native-jdbc:2.7.1\"\n",
    "]\n",
    "ram = 12\n",
    "cpu = 12*3\n",
    "# Define the application name and setup session\n",
    "appName = \"Connect To ClickHouse via PySpark\"\n",
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         # .config(\"spark.jars.packages\", \"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3,com.clickhouse:clickhouse-jdbc:0.6.0,com.clickhouse:clickhouse-http-client:0.6.0,org.apache.httpcomponents.client5:httpclient5:5.3.1,com.github.housepower:clickhouse-native-jdbc:2.7.1\")\n",
    "         .config(\"spark.jars.packages\", ','.join(packages))\n",
    "         .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         # .config(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.host\", \"34.118.21.172\") # from GCP VM instances or other server ip\n",
    "         .config(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.password\", \"1234\")\n",
    "         .config(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "         # .config(\"spark.spark.clickhouse.write.compression.codec\", \"lz4\")\n",
    "         # .config(\"spark.clickhouse.read.compression.codec\", \"lz4\")\n",
    "         # .config(\"spark.clickhouse.write.format\", \"arrow\")\n",
    "         #    .config(\"spark.clickhouse.write.distributed.convertLocal\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.repartitionNum\", \"1\") \n",
    "         #.config(\"spark.clickhouse.write.maxRetry\", \"1000\")\n",
    "         #    .config(\"spark.clickhouse.write.repartitionStrictly\", \"true\") \n",
    "         #    .config(\"spark.clickhouse.write.distributed.useClusterNodes\", \"false\") \n",
    "        # .config(\"spark.clickhouse.write.batchSize\", \"10000000\")\n",
    "         # .config(\"spark.sql.catalog.clickhouse.socket_timeout\", \"600000000\")\n",
    "         # .config(\"spark.sql.catalog.clickhouse.connection_timeout\", \"600000000\")\n",
    "         # .config(\"spark.sql.catalog.clickhouse.query_timeout\", \"600000000\")\n",
    "         # .config(\"spark.clickhouse.options.socket_timeout\", \"600000000\")\n",
    "         # .config(\"spark.clickhouse.options.connection_timeout\", \"600000000\")\n",
    "         # .config(\"spark.clickhouse.options.query_timeout\", \"600000000\")\n",
    "         .config(\"spark.executor.memory\", f\"{ram}g\")\n",
    "         #.config(\"spark.executor.cores\", \"5\")\n",
    "         .config(\"spark.driver.maxResultSize\", f\"{ram}g\")\n",
    "         .config(\"spark.driver.memory\", f\"{ram}g\")\n",
    "         .config(\"spark.executor.memoryOverhead\", f\"{ram}g\")\n",
    "         #.config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse\", \"xenon.clickhouse.ClickHouseCatalog\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "# spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "spark.sql(\"use clickhouse\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:43:28.863441Z",
     "start_time": "2024-06-02T18:43:10.108915Z"
    }
   },
   "id": "55e195abbdef252c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:44:58.909889Z",
     "start_time": "2024-05-11T08:44:58.819819Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "f09d2a8b16e81b7b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql('show databases').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T08:45:09.180380Z",
     "start_time": "2024-05-11T08:45:06.313937Z"
    }
   },
   "id": "73af3bed97389096",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#spark.sql('CREATE DATABASE first_database').show()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:41:11.054458Z",
     "start_time": "2024-04-28T08:41:09.906345Z"
    }
   },
   "id": "c588a2c1e5297196",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df = spark.read.csv('/projects/app/stations.csv',header=True,inferSchema=True)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:41:24.442985Z",
     "start_time": "2024-04-28T08:41:18.846815Z"
    }
   },
   "id": "12338253e0fb73b2",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T08:55:54.433955Z",
     "start_time": "2024-04-28T08:55:53.102084Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.sql('''SELECT * FROM first_database.WeatherData''')",
   "id": "649c6cd87e47d045",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T09:01:32.922226Z",
     "start_time": "2024-04-28T09:01:32.520627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:clickhouse://34.118.0.142:9000/\")\n",
    "    .option(\"driver\", \"com.github.housepower.jdbc.ClickHouseDriver\")\n",
    "    .option(\"user\", \"default\")\n",
    "    .option(\"password\", \"1234\")\n",
    "    .option(\"dbtable\", \"first_database.WeatherDataSpark\")\n",
    "    .option(\"batchsize\", \"10000000\")\n",
    "    .option(\"numPartitions\", cpu)\n",
    "    .option(\"truncate\", \"true\")\n",
    "    .option(\"isolationLevel\", \"NONE\")\n",
    "    .save()\n",
    ")\n"
   ],
   "id": "2172f42607251671",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView('df_sql')\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO first_database.WeatherDataSpark\n",
    "SELECT * FROM df_sql\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:55:59.077529Z",
     "start_time": "2024-04-28T08:55:54.436213Z"
    }
   },
   "id": "538d32f1df29876b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM default.input_test_table\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e04e588a733fe52",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#df.createOrReplaceTempView('df_sql')\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT * FROM first_database.Weather_moscow_archive\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T08:50:36.023064Z",
     "start_time": "2024-05-11T08:50:35.485428Z"
    }
   },
   "id": "11249a781edea261",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:53:46.347435Z",
     "start_time": "2024-05-11T08:53:46.205177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT * FROM first_database.Weather_moscow_archive\n",
    "\"\"\")"
   ],
   "id": "f9bb5156d7410fe9",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:54:00.541835Z",
     "start_time": "2024-05-11T08:54:00.219444Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "f6119baf2692867f",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T08:53:33.144509Z",
     "start_time": "2024-05-11T08:53:33.140366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание сессии Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Temperature Forecasting\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Загрузка данных из базы данных в Spark DataFrame\n",
    "# jdbc_url = \"jdbc:postgresql://your_database_host:your_database_port/your_database_name\"\n",
    "# table_name = \"your_table_name\"\n",
    "# user = \"your_username\"\n",
    "# password = \"your_password\"\n",
    "# \n",
    "# df = spark.read.format(\"jdbc\") \\\n",
    "#     .option(\"url\", jdbc_url) \\\n",
    "#     .option(\"dbtable\", table_name) \\\n",
    "#     .option(\"user\", user) \\\n",
    "#     .option(\"password\", password) \\\n",
    "#     .load()\n",
    "\n",
    "# Предобработка данных\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "\n",
    "df = df.withColumn(\"Local time in Moscow\", to_timestamp(\"Local time in Moscow\", \"dd.MM.yyyy HH:mm\")) \\\n",
    "    .filter(df[\"Local time in Moscow\"] < \"2023-01-01\")\n",
    "\n",
    "df = df.withColumn(\"year\", year(\"Local time in Moscow\")) \\\n",
    "    .withColumn(\"month\", month(\"Local time in Moscow\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"Local time in Moscow\")) \\\n",
    "    .withColumn(\"hour\", hour(\"Local time in Moscow\")) \\\n",
    "    .select(\"year\", \"month\", \"day\", \"hour\", \"T\")\n",
    "\n",
    "# Обучение модели с помощью CatBoost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(df.drop(\"T\"), df[\"T\"])\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "# Процесс предобработки и предсказания на тестовых данных аналогичен процессу обучения\n",
    "# ...\n",
    "\n",
    "# Закрытие сессии Spark\n",
    "spark.stop()\n"
   ],
   "id": "9bce943ded46a7a3",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "downloading data and fitting model",
   "id": "58a0a12de559ae1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:31:52.048856Z",
     "start_time": "2024-05-25T08:31:51.763074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clickhouse_df = spark.sql(\"\"\"\n",
    "SELECT * FROM first_database.Weather_moscow_archive\n",
    "\"\"\")"
   ],
   "id": "b6983b56ad75d1ee",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:32:24.222552Z",
     "start_time": "2024-05-25T08:32:24.008500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clickhouse_df = clickhouse_df.withColumn(\"Local time in Moscow\", F.to_timestamp(F.col(\"Local time in Moscow\"), 'dd.MM.yyyy HH:mm'))\n",
    "clickhouse_df.show()\n"
   ],
   "id": "3f6686b44908ffa2",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:34:04.986248Z",
     "start_time": "2024-05-25T08:33:43.047431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "clickhouse_df = spark.sql(\"\"\"\n",
    "SELECT * FROM first_database.Weather_moscow_archive\n",
    "\"\"\")\n",
    "# Обработка данных и подготовка их для обучения модели\n",
    "clickhouse_df = clickhouse_df.withColumn(\"Local time in Moscow\", F.to_timestamp(F.col(\"Local time in Moscow\"), 'dd.MM.yyyy HH:mm'))\n",
    "\n",
    "processed_df = clickhouse_df \\\n",
    "    .withColumn(\"year\", F.year(F.col(\"Local time in Moscow\"))) \\\n",
    "    .withColumn(\"month\", F.month(F.col(\"Local time in Moscow\"))) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(F.col(\"Local time in Moscow\"))) \\\n",
    "    .withColumn(\"hour\", F.hour(F.col(\"Local time in Moscow\"))) \\\n",
    "    .select(\"year\", \"month\", \"day\", \"hour\", \"T\",\"Local time in Moscow\")\n",
    "\n",
    "# Конвертация Spark DataFrame в Pandas DataFrame\n",
    "pandas_df = processed_df.toPandas()\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "feature_columns = [\"year\", \"month\", \"day\", \"hour\"]\n",
    "label_column = \"T\"\n",
    "\n",
    "# Создание и обучение модели CatBoost\n",
    "catboost_model = CatBoostRegressor()\n",
    "catboost_model.fit(pandas_df[feature_columns], pandas_df[label_column])\n",
    "pandas_df"
   ],
   "id": "81500e4ac475c89e",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:14:11.613547Z",
     "start_time": "2024-05-25T08:14:08.686897Z"
    }
   },
   "cell_type": "code",
   "source": "clickhouse_df.show()",
   "id": "a0d2f4ade44d2c6",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:34:37.144352Z",
     "start_time": "2024-05-25T08:34:37.136890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = ['2024-06-28 21:00:00']\n",
    "\n",
    "def temp_pred_catb(data,model):\n",
    "    # Преобразуйте новые данные в DataFrame и укажите правильное имя столбца\n",
    "    data_df = pd.DataFrame(data, columns=['Local time in Moscow'])\n",
    "    data_df['Local time in Moscow'] = pd.to_datetime(data_df['Local time in Moscow'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Извлеките признаки из новых данных\n",
    "    data_features = pd.DataFrame()\n",
    "    data_features['year'] = data_df['Local time in Moscow'].dt.year\n",
    "    data_features['month'] = data_df['Local time in Moscow'].dt.month\n",
    "    data_features['day'] = data_df['Local time in Moscow'].dt.day\n",
    "    data_features['hour'] = data_df['Local time in Moscow'].dt.hour\n",
    "    print(data_features)\n",
    "    # Сделайте предсказания\n",
    "    predictions = model.predict(data_features)\n",
    "    return predictions[0]"
   ],
   "id": "9bdeb6087eb9a33b",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T08:34:39.020776Z",
     "start_time": "2024-05-25T08:34:39.002450Z"
    }
   },
   "cell_type": "code",
   "source": "temp_pred_catb(new_data,catboost_model)",
   "id": "31adac8fbf291b6e",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:13:40.132466Z",
     "start_time": "2024-05-12T21:13:40.119701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "  # Сохранение модели\n",
    "joblib.dump(catboost_model, 'my_spark_catb_model.joblib')"
   ],
   "id": "b8f9ca2d336fa1cf",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:13:41.002357Z",
     "start_time": "2024-05-12T21:13:40.976505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка модели\n",
    "loaded_model = joblib.load('my_spark_catb_model.joblib')\n",
    "# Загрузите новые данные и предобработайте их, извлекая признаки\n",
    "# Пример: новые данные хранятся в переменной new_data\n",
    "new_data = ['2024-07-30 21:00:00']\n",
    "\n"
   ],
   "id": "3c1a47d61ce0ab89",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:13:43.412877Z",
     "start_time": "2024-05-12T21:13:43.400472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def temp_pred(data,model):\n",
    "    # Преобразуйте новые данные в DataFrame и укажите правильное имя столбца\n",
    "    data_df = pd.DataFrame(data, columns=['Local time in Moscow'])\n",
    "    data_df['Local time in Moscow'] = pd.to_datetime(data_df['Local time in Moscow'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Извлеките признаки из новых данных\n",
    "    data_features = pd.DataFrame()\n",
    "    data_features['year'] = data_df['Local time in Moscow'].dt.year\n",
    "    data_features['month'] = data_df['Local time in Moscow'].dt.month\n",
    "    data_features['day'] = data_df['Local time in Moscow'].dt.day\n",
    "    data_features['hour'] = data_df['Local time in Moscow'].dt.hour\n",
    "\n",
    "    # Сделайте предсказания\n",
    "    predictions = model.predict(data_features)\n",
    "    return predictions[0]"
   ],
   "id": "14ad94546d746fb",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:13:44.457218Z",
     "start_time": "2024-05-12T21:13:44.433547Z"
    }
   },
   "cell_type": "code",
   "source": "temp_pred(data=new_data, model=loaded_model)",
   "id": "e89cd716d9eb666b",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### testing join from 2 tables",
   "id": "f5e8c76fc6aaf4ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:49:56.612064Z",
     "start_time": "2024-05-17T07:49:56.226458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df.createOrReplaceTempView('df_sql')\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT localtime, Temperature_C FROM first_database.WeatherData \n",
    "ORDER BY localtime \n",
    "DESC LIMIT 1\"\"\")\n",
    "\n",
    "# Получите временную метку из DataFrame\n",
    "timestamp = df.collect()[0][0]\n",
    "Temperature_C = df.collect()[0][1]\n",
    "# Преобразуйте временную метку в строку в нужном формате\n",
    "formatted_date = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Создайте словарь\n",
    "X_dict = {'date': [formatted_date]}\n"
   ],
   "id": "84c165bc99109ac",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:33:52.478263Z",
     "start_time": "2024-05-17T07:33:47.414451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_data_response_xgb = requests.post('https://weatherimage-ma4ayonvha-lm.a.run.app/model/predict_xgb_t', json=X_dict) # local address\n",
    "\n",
    "print(predicted_data_response_xgb)\n",
    "print(predicted_data_response_xgb.text)"
   ],
   "id": "ac25701ae588c9fd",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:37:07.611576Z",
     "start_time": "2024-05-17T07:37:07.572502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from io import StringIO\n",
    "\n",
    "predicted_results_xgb = pd.read_json(StringIO(predicted_data_response_xgb.json()))"
   ],
   "id": "b1fb1984212693f5",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:39:39.327205Z",
     "start_time": "2024-05-17T07:39:36.735085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_data_response_catb = requests.post('https://weatherimage-ma4ayonvha-lm.a.run.app/model/predict_catb_t', json=X_dict) # local address\n",
    "\n",
    "print(predicted_data_response_catb)\n",
    "print(predicted_data_response_catb.text)"
   ],
   "id": "d0ce4e84b873108e",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:53:03.442610Z",
     "start_time": "2024-05-17T07:53:03.409774Z"
    }
   },
   "cell_type": "code",
   "source": "predicted_results_catb = pd.read_json(StringIO(predicted_data_response_catb.json()))\n",
   "id": "a873f0dd85dcf69b",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:51:03.510564Z",
     "start_time": "2024-05-17T07:51:03.494884Z"
    }
   },
   "cell_type": "code",
   "source": "round(predicted_results_xgb.iloc[0,0],2)",
   "id": "eca7cb816058d43e",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:53:10.134433Z",
     "start_time": "2024-05-17T07:53:10.110291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_comparsion = pd.DataFrame()\n",
    "data_comparsion['Localtime'] = [formatted_date]\n",
    "data_comparsion['Temperature_C'] = [Temperature_C]\n",
    "data_comparsion['Temperature_xgb_C'] = [round(predicted_results_xgb.iloc[0,0],2)]\n",
    "data_comparsion['Temperature_catb_C'] = [round(predicted_results_catb.iloc[0,0],2)]\n",
    "data_comparsion"
   ],
   "id": "ac5230a9597a1cec",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:55:06.353021Z",
     "start_time": "2024-05-17T07:55:06.322031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_comparsion.createOrReplaceTempView('data_comparsion')\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO first_database.WeatherComparsion\n",
    "SELECT * FROM data_comparsion\n",
    "\"\"\")"
   ],
   "id": "8ac16190aa2ffde3",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:56:12.873475Z",
     "start_time": "2024-05-17T07:56:12.835296Z"
    }
   },
   "cell_type": "code",
   "source": "data_comparsion.write.mode(\"overwrite\").insertInto(\"first_database.WeatherComparsion\")",
   "id": "61ee78b138bc2635",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:44:23.535237Z",
     "start_time": "2024-05-18T07:44:09.062141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "    \n",
    "# Выполните запрос к базе данных для получения последней записи\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT localtime, Temperature_C FROM first_database.WeatherData \n",
    "ORDER BY localtime \n",
    "DESC LIMIT 1\"\"\")\n",
    "\n",
    "# Получите временную метку и температуру из DataFrame\n",
    "timestamp, temperature = df.collect()[0]\n",
    "\n",
    "# Преобразуйте временную метку в нужный формат строки\n",
    "formatted_date = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Создайте словарь для отправки запроса модели\n",
    "X_dict = {'date': [formatted_date]}\n",
    "\n",
    "# Отправьте запрос к модели для прогнозирования\n",
    "predicted_data_response_xgb = requests.post('https://weatherimage-ma4ayonvha-lm.a.run.app/model/predict_xgb_t', json=X_dict)\n",
    "predicted_data_response_catb = requests.post('https://weatherimage-ma4ayonvha-lm.a.run.app/model/predict_catb_t', json=X_dict)\n",
    "\n",
    "\n",
    "predicted_results_xgb = pd.read_json(StringIO(predicted_data_response_xgb.json()))\n",
    "predicted_results_catb = pd.read_json(StringIO(predicted_data_response_catb.json()))\n",
    "\n",
    "\n",
    "\n",
    "# Создайте DataFrame для сравнения данных\n",
    "data_comparison = pd.DataFrame({\n",
    "    'Localtime': [formatted_date],\n",
    "    'Temperature_C': [temperature],\n",
    "    'Temperature_xgb_C': [round(predicted_results_xgb.iloc[0,0],2)],\n",
    "    'Temperature_catb_C': [round(predicted_results_catb.iloc[0,0],2)]\n",
    "})\n",
    "\n",
    "data_comparison\n"
   ],
   "id": "75cad2ea1422af8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Inserting archive update\n",
    " local kernel"
   ],
   "id": "366cf9f096e01317"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T11:47:37.629950Z",
     "start_time": "2024-05-30T11:47:23.688852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "csv_file_path = 'data/archive.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Преобразование столбцов с датами в нужный формат\n",
    "df['Local time in Moscow'] = pd.to_datetime(df['Local time in Moscow'], format='%d.%m.%Y %H:%M')\n",
    "\n",
    "# Удаление столбца ff10\n",
    "df.drop(columns=['ff10','P','Pa', 'ff3', 'N', 'WW', 'W1', 'W2', 'Tn', 'Tx', 'Cl', 'H', 'Cm', 'Ch',\n",
    "                 'Td', 'tR', 'Tg', 'RRR', 'E', \"E'\", 'sss'], inplace=True)\n",
    "\n",
    "# Преобразование числовых столбцов в тип float\n",
    "float_columns = ['T', 'Po',  'U', 'Ff',   ]\n",
    "for col in float_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "# Преобразование строковых столбцов в тип string и обработка NaN значений\n",
    "string_columns = ['DD', 'Nh',  'VV', ]\n",
    "for col in string_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(object).where(pd.notnull(df[col]), None)\n",
    "\n",
    "# Подключение к ClickHouse через TCP (порт 9000)\n",
    "client = Client(host='34.118.21.172', port=9000, user='default', password='1234', database='first_database')\n",
    "\n",
    "# Преобразование данных DataFrame в список кортежей\n",
    "data = [tuple(row) for row in df.itertuples(index=False, name=None)]\n",
    "\n",
    "# Определение таблицы и колонок (если еще не создана таблица, создайте ее предварительно)\n",
    "table_name = 'first_database.Weather_moscow_archive_updated'\n",
    "columns = ', '.join([f'`{col}`' for col in df.columns])\n",
    "\n",
    "# Вставка данных в таблицу ClickHouse\n",
    "client.execute(f'INSERT INTO {table_name} ({columns}) VALUES', data, types_check=True)\n",
    "\n",
    "print(\"Данные успешно записаны в ClickHouse\")\n"
   ],
   "id": "40cdc907b9e6148f",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "print()",
   "id": "2755dfa71748ccf3",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
